from rag_pipeline.vector_store import CocktailVectorStore
from rag_pipeline.llm_interface import LocalLLM
from rag_pipeline.user_memory import UserMemory


class RAGEngine:
    """
    Combines a vector store, memory, and a local LLM to answer questions using RAG.
    """

    def __init__(
        self,
        vector_store: CocktailVectorStore,
        llm: LocalLLM,
        memory: UserMemory
    ):
        """
        Initialize RAG engine with index, LLM, and user memory.

        :param vector_store: CocktailVectorStore instance for document retrieval.
        :param llm: LocalLLM instance for response generation.
        :param memory: UserMemory instance for storing user preferences.
        """
        self.vector_store = vector_store
        self.llm = llm
        self.memory = memory

    def run(self, user_query: str) -> str:
        """
        Process a user's query using the RAG pipeline with memory support.

        :param user_query: Natural language question from the user.
        :return: Answer generated by the LLM.
        """
        # Step 1: Try to extract new ingredients from the query
        self.memory.add_ingredients_from_text(user_query)

        # Step 2: Modify the query if it references favorites
        if "my favorite" in user_query.lower():
            favorites = self.memory.get_favorites()
            if favorites:
                # Rebuild the query using the actual ingredients
                fav_str = ", ".join(favorites)
                user_query = f"Recommend cocktails that include: {fav_str}"

        # Step 3: Retrieve context and ask LLM
        context = self.vector_store.query(user_query)

        # addition: if no context
        if not context.strip():
            return "Sorry, I don't know the answer. I couldn't find any relevant cocktails."

        # Step 4: Ask LLM with context
        answer = self.llm.ask(context=context, question=user_query)
        return answer